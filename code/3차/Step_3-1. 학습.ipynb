{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PET 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/src')\n",
    "sys.path.append(r'C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/trained_pet모델')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions\n",
    "import pet.modeling\n",
    "import pet.wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/data3/df_train.csv\")\n",
    "df_test = pd.read_csv(\"C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/data3/df_test.csv\")\n",
    "df_unlabeled = pd.read_csv(\"C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/data3/df_unlabeled.csv\")\n",
    "df_eval = pd.read_csv(\"C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/data3/df_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.astype(str)\n",
    "df_test = df_test.astype(str)\n",
    "df_unlabeled = df_unlabeled.astype(str)\n",
    "df_eval = df_eval.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = np.array(['1', '2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개별 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvp_numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training patterns: 100%|██████████| 1/1 [05:15<00:00, 315.40s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:19<00:00, 259.85s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:20<00:00, 260.56s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:19<00:00, 259.46s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:19<00:00, 259.28s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:20<00:00, 260.94s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:20<00:00, 260.71s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:18<00:00, 258.78s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:16<00:00, 256.53s/it]\n",
      "Training patterns: 100%|██████████| 1/1 [04:19<00:00, 259.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# model_paths = []\n",
    "# for i in pvp_numbers:\n",
    "#     pvp_numbers = [i]\n",
    "#     output_dir=f'C:/Users/user/Desktop/PET_trained_model/PVP{i}/'\n",
    "#     model_path = helper_functions.train_models(pvp_numbers, df_train['text'], df_train['label'], df_eval['text'], df_eval['label'], 'yelp-polarity', label_list, output_dir=output_dir)\n",
    "#     model_paths.append(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['C:/Users/user/Desktop/PET_trained_model/PVP0/p0-i0',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP1/p1-i0',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP2/p2-i8',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP3/p3-i1',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP4/p4-i1',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP5/p5-i9',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP6/p6-i0',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP7/p7-i4',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP8/p8-i0',\n",
    "                'C:/Users/user/Desktop/PET_trained_model/PVP9/p9-i0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 모델 별 점수 내기(scores 내는 데이터 1:test data, 2:unlabel data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "scores_test = helper_functions.score_models(\n",
    "    model_paths, df_test['text'], df_test['label'], label_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "scores_unlabel = helper_functions.score_models(\n",
    "    model_paths, df_unlabeled['text'], df_unlabeled['label'], label_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = [0.8834, 0.9292, 0.9406, 0.9636, 0.9424, 0.9627, 0.9223, 0.7962, 0.5476, 0.8564]\n",
    "scores_unlabel = [0.8842, 0.9341, 0.9403, 0.9637, 0.9463, 0.9639, 0.9273, 0.8056, 0.5426, 0.8554]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_test : [0.8834, 0.9292, 0.9406, 0.9636, 0.9424, 0.9627, 0.9223, 0.7962, 0.5476, 0.8564]\n",
      "scores_unlabel : [0.8842, 0.9341, 0.9403, 0.9637, 0.9463, 0.9639, 0.9273, 0.8056, 0.5426, 0.8554]\n"
     ]
    }
   ],
   "source": [
    "print('scores_test :', scores_test)\n",
    "print('scores_unlabel :', scores_unlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론 및 라벨링(개별)\n",
    "score는 아무거나 해도 상관없음. 어차피 모델 1개라 앙상블 안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "labeling_accuracy_PVP = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    model_path = [model_paths[i]]\n",
    "    score_test = [scores_test[i]]\n",
    "    \n",
    "    unlabeled_probabilities = helper_functions.ensemble_predict(model_path, score_test, df_unlabeled['text'])\n",
    "    acc = helper_functions.score_predictions(df_unlabeled['label'], unlabeled_probabilities, label_list)\n",
    "    labeling_accuracy_PVP.append(acc)\n",
    "    np.save(f'C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/data3/labeling/unlabeled_probabilities_PVP{i}.npy', unlabeled_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8842,\n",
       " 0.9341,\n",
       " 0.9403,\n",
       " 0.9637,\n",
       " 0.9463,\n",
       " 0.9639,\n",
       " 0.9273,\n",
       " 0.8056,\n",
       " 0.5426,\n",
       " 0.8554]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeling_accuracy_PVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론 및 라벨링(모델별)\n",
    "\n",
    "unlabeled_probabilities_1 : scores_test로 가중치 설정하여 앙상블\n",
    "\n",
    "unlabeled_probabilities_2 : scores_unlabel로 가중치 설정하여 앙상블\n",
    "\n",
    "random1 = [0, 2, 4]\n",
    "\n",
    "random2 = [1, 3, 6]\n",
    "\n",
    "random3 = [5, 8, 9]\n",
    "\n",
    "cosine = [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nums = [[0, 2, 4], [1, 3, 6], [5, 8, 9], [3, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_accuracy_test_model = []\n",
    "labeling_accuracy_unlabel_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "for i in model_nums:\n",
    "    model_num = i\n",
    "    models_path = []\n",
    "    models_score_test = []\n",
    "    models_score_unlabel = []\n",
    "    \n",
    "    for j in model_num:\n",
    "        model_path = model_paths[j]\n",
    "        model_score_test = scores_test[j]\n",
    "        model_score_unlabel = scores_unlabel[j]\n",
    "        \n",
    "        models_path.append(model_path)\n",
    "        models_score_test.append(model_score_test)\n",
    "        models_score_unlabel.append(model_score_unlabel)\n",
    "        \n",
    "    unlabeled_probabilities_1 = helper_functions.ensemble_predict(models_path, models_score_test, df_unlabeled['text'])\n",
    "    acc_1 = helper_functions.score_predictions(df_unlabeled['label'], unlabeled_probabilities_1, label_list)\n",
    "    labeling_accuracy_test_model.append(acc_1)\n",
    "    np.save(f'C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/data3/labeling/unlabeled_probabilities_model{i}_test.npy', unlabeled_probabilities_1)\n",
    "    \n",
    "    unlabeled_probabilities_2 = helper_functions.ensemble_predict(models_path, models_score_unlabel, df_unlabeled['text'])\n",
    "    acc_2 = helper_functions.score_predictions(df_unlabeled['label'], unlabeled_probabilities_2, label_list)\n",
    "    labeling_accuracy_unlabel_model.append(acc_2)\n",
    "    np.save(f'C:/Users/user/Desktop/논문리뷰/Pattern-Exploiting-Training(PET)/code/testPET - 복사본/data3/labeling/unlabeled_probabilities_model{i}_unlabel.npy', unlabeled_probabilities_2)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9595, 0.9568, 0.9633, 0.9686]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeling_accuracy_test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9595, 0.9567, 0.9632, 0.9686]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeling_accuracy_unlabel_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY39_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
